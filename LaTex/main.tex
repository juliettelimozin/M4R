\documentclass[12pt,twoside]{article}
\usepackage{indentfirst}
\usepackage[nottoc]{tocbibind}
\usepackage{fancyhdr,ragged2e}
\usepackage{todonotes}
\fancyhead{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Definitions for the title page
% Edit these to provide the correct information
% e.g. \newcommand{\reportauthor}{Timothy Kimber}

\newcommand{\reporttitle}{Evaluating the Use of Machine Learning for Doubly-Robust Estimation in Missing Data}
\newcommand{\reportauthor}{Juliette Maiko Limozin}
\newcommand{\supervisor}{Dr David Whitney}
\newcommand{\degreetype}{Mathematics}
\newcommand{\expit}{\text{expit}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% load some definitions and default packages
\input{includes}

% load some macros
\input{notation}

\date{October 2020}

\begin{document}

% load title page
\input{titlepage}


% page numbering etc.
\pagenumbering{roman}
\clearpage{\pagestyle{empty}\cleardoublepage}
\setcounter{page}{1}
\pagestyle{fancy}
\setlength{\parindent}{5ex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Your abstract.cddvfvfbb


\end{abstract}

\cleardoublepage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgments}
I would like to thank my project supervisor, Dr David Whitney, for his continued guidance and support throughout this project. Our weekly meetings have always been a pleasure, and his advice on postgraduate studies have also been of immense help. 

I would also like to thank my family, who even though might not understand Statistics at a Master's level, have always enabled me to achieve my best.

Last but not least, I would like to thank my various statistics lecturers who, in my 4 years at Imperial, have enamoured me with their passion for Statistics.

\clearpage{\pagestyle{empty}\cleardoublepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--- table of contents
\tableofcontents 


\clearpage{\pagestyle{empty}\cleardoublepage}
\pagenumbering{arabic}
\setcounter{page}{1}
\fancyhead[L]{\textsl{\leftmark}}

\setcitestyle{authoryear,open={(},close={)}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction} 
\todo[inline]{Finish introduction}

- Clinical trials often have reserve patients because the trial patients might drop out for x and y reason

- often the calculation of the mean outcome of the trial is needed, but what do we do with the patients we don't have an outcome for?

- in this report we will cover the Doubly-Robust mean estimation method combined with machine learning tools that will help generate a consistent estimation of the mean thanks to easy to use computational tools

- Hopefully this report can be useful to upcoming clinical trials in making them understand how the machine learning models can be use to improve mean estimation from missing data

\section{Setting} 

Suppose we have a data set of size $n$, and realisations of random vector $(\mathbf{Y}, \mathbf{W}, \mathbf{R})$ where $\mathbf{Y}$ denotes a partially denoted outcome, i.e. $Y_i$ is missing for individual $i$'s unobserved outcome. $\mathbf{W}$ denotes the auxiliary variables, and $\mathbf{R}$ is an indicator on whether the $i$th individual's outcome is missing or not; that is, if $R_i = 1$, the outcome is observed, whereas $R_i = 0$ would indicate that the outcome is missing.
To summarise:
\begin{itemize}
    \item $\mathbf{Y}$: partially observed outcome 
    \item $\mathbf{W}$: auxiliary variables/covariates 
    \item $\mathbf{R}$: indicator on whether $Y$ is observed or not ($R = 1$: observed, $R = 0$: missing) 
\end{itemize}

A data set is said to be \textbf{missing at random (MAR)} if the conditional probability that a particular missingness pattern occurs given the data is independent of the missing values in that pattern \citep{vansteelandt}, i.e $P(R=r|\mathbf{W}, Y) = P(R= r|\mathbf{W})$ and  $P(Y=y|\mathbf{W}, R) = P(Y=y|\mathbf{W})$. So in the example of a clinical trial, we would consider the data set obtained from the study to be MAR if given the measurements taken from the patients, the reasons why some patients dropped out were unrelated to the outcome of the trial. The MAR setting will therefore have effects on any causal inference we may perform on this data set.

When the missingness pattern is independent of both the auxiliary variables and the outcome data, the data set is said to be \textbf{missing completely at random (MCAR)}.\\

We make the assumption that $(W_1, Y_1, R_1),...,(W_n, Y_n, R_n)$ are independent and identically distributed for the rest of the report. We also assume that $R$ is independent of $Y$ given W so $(\mathbf{W}_1, Y_1, R_1), ... ,(\mathbf{W}_n, Y_n, R_n)$ are MAR (\citeauthor{vansteelandt}), where the index $i = 1,...,n$ denotes the $i$th individual of the data.\\

\section{Inverse probability weighting and doubly robust estimators}

\subsection{Inverse Probability Weighting approach}

Suppose we wish to calculate or estimate the outcome mean $\beta = E(\mathbf{Y})$. A problem occurs when trying to compute it as $\mathbf{Y}$ is only partially observed, so calculating a mean with only the observed outcomes would not be a good representation of the data outcome as a whole.

One solution to this problem is the Inverse Probability Weighting (IPW) approach. Its name comes from the fact that each observed outcome $Y_i$ (called \textit{complete} case) is weighted by the probability that the $i$th individual is observed given the set of variables for this outcome, $\mathbf{W}_i$ (\citeauthor{vansteelandt}). We denote this weight by
\begin{align*}
    \pi(\mathbf{W})= E(R|\mathbf{W}) = P(R = 1|\mathbf{W}).
\end{align*}

We assume that the weighting is positive for any $\mathbf{W}_i$.

The IPW estimator of the mean $\beta$ is then simply 
\begin{equation} \label{IPW_est}
    \hat{\beta}_{IPW} = \frac{1}{n} \sum_{i=1}^{n} \frac{R_iY_i}{\pi(\mathbf{W}_i)}.
\end{equation}

As $\pi(\mathbf{W})$ is unknown as the data is missing at random, we propose a propensity model, most commonly a parametric model $\pi(\mathbf{W}; \boldsymbol\alpha)$ for the missingness pattern and we build and estimator $\hat{\boldsymbol\alpha}$ of $\boldsymbol\alpha$ from the data (\citeauthor{vansteelandt}).

The IPW estimator is consistent for $\beta$ if the propensity model $\pi(\mathbf{W}; \boldsymbol\alpha)$ is correctly specified \citep{davidian}. 

If the propensity model $\pi(\mathbf{W})$ is correctly specified, then by the weak law of large numbers (WLLN), $\hat{\beta}_{IPW}$ will converges in probability to the true mean $\beta$, so this makes the IPW estimate consistent for $\beta$. Using the Central Limit Theorem (CLT), we have $\sqrt{n}(\hat{\beta}_{IPW}-\beta) \xrightarrow{d} N(0,\sigma_1^2)$ where $\sigma$ is the estimated outcome variance, so we can use this result to perform inference tests and confidence intervals for the true mean.

A quick drawback from the IPW estimator in \ref{IPW_est} is that it is usually difficult to construct at propensity model unless the auxiliary variables $\mathbf{W}$ are categorical or the sample size $n$ is large enough, so a removal of bias is not guaranteed by the IPW estimator when he model is misspecified. Another problem that arises is unstable weights: if the model is misspecified, the fitted probability of observing an outcome given the auxiliary variables may be very small for the majority of individuals, and very large for a select few, so the IPW estimator is mainly dominated by these large weights \citep{seaman}.

\citet{kang} suggest that the model being misspecified is more likely the cause of large weights, rather than if the auxiliary variables are truly predictive of whether the outcome may be missing or not. There exist a few tests, including the \citet{hosmer} test, to detect poor fit of the propensity model.\\

\subsection{Regression imputation approach} 

Alternatively, one could make use of the conditional expectation of the outcome 
\begin{align*}
    m(\mathbf{W}) = E(Y|\mathbf{W})
\end{align*}
to have a different estimator of the mean \citep{davidian,vansteelandt}, sometimes called the regression imputation estimator: 

\begin{equation}
    \hat{\beta}_ {RI} = \frac{1}{n}\sum_{i = 1}^n m(\mathbf{W}_i)
\end{equation}

Because again the true value of $m(\mathbf{W})$ is unknown, we propose an outcome model, most commonly a parametric model $m(\mathbf{W}, \gamma)$ with some parameter $\gamma$, and again we build an estimator $\hat{\gamma}$ for $\gamma$ from the sample data. Again, like in IPW, the efficiency of the the RI estimator relies on the correct specification of the outcome model, as then the WLLN would apply and so the RI estimator would be consistent for $\beta$. \\

\subsection{Augmented IPW approach} 

However this motivates the idea behind the augmented IPW estimator. One could think of combining the ideas from the IPW and RI estimators. From \citet{davidian}, estimators of $\beta$, when the propensity model is correctly specified and the estimators are consistent and are asymptotically normal(i.e. CLT holds), are equivalent to 

\begin{equation}
    \frac{1}{n}\sum_{i=1}^{n}\frac{R_iY_i}{\pi(\mathbf{W}_i, \hat{\alpha})} - \frac{1}{n}\sum_{i=1}^{n} \left(1 - \frac{R_i}{\pi(\mathbf{W_i},\hat{\alpha})} \right) h(\mathbf{W}_i)
\end{equation}

This is called an augmented IPW as it is the sum of the IPW estimator and an augmentation term depending on $h(\mathbf{W})$ \citep{davidian}. By taking $-h(\mathbf{W}) = m(\mathbf{W}, \hat{\gamma})$ like in the RI approach, we have a new estimator
\begin{equation}
    \hat\beta_{DR} = \frac{1}{n}\sum_{i=1}^{n}\frac{R_iY_i}{\pi(\mathbf{W}_i, \hat{\alpha})} + \frac{1}{n}\sum_{i=1}^{n} \left(1 - \frac{R_i}{\pi(\mathbf{W_i},\hat{\alpha})} \right) m(\mathbf{W}_i, \hat\gamma)
\end{equation}

$\hat\beta_{DR}$ is consistent (i.e $\hat\beta_{DR}$ converges in probability to the true mean $\beta$) and asymptotically normal when either the propensity score or outcome model are correctly specified, as the correct specification of one suffices for consistency, and therefore the CLT to hold for $\hat\beta_{DR}$, which is why we call this a \textbf{doubly-robust estimator} for $\beta$. This allows us to not require a correct specification for the entire outcome ans missingness distributions whilst maintaining a better efficiency than IPW estimators \citep{bangrobins,vansteelandt}.

For example, doubly-robust estimators are used to estimate the mean of clinical trial outcome when there have been dropouts mid-trial; we use acquired variables/observations on patients to compute a DR estimation of the mean. \\

The proof of the doubly-robustness of this mean estimation is as follows, mainly taken from \citet{vansteelandt}.

Rewriting $\hat{\beta}_{DR}$, we use the WLLN to get
\begin{align*}
    \hat{\beta}_{DR} & = \frac{1}{n}\sum_{i=1}^{n}\frac{R_iY_i}{\pi(\mathbf{W}_i, \hat{\alpha})} + \frac{1}{n}\sum_{i=1}^{n} \left(1 - \frac{R_i}{\pi(\mathbf{W_i},\hat{\alpha})} \right) m(\mathbf{W}_i, \hat\gamma) \\
    & = \frac{1}{n}\sum_{i=1}^{n} Y_i + \frac{1}{n}\sum_{i=1}^{n}\left(1 - \frac{R_i}{\pi(\mathbf{W_i},\hat{\alpha})} \right) (m(\mathbf{W}_i, \hat\gamma)-Y_i) \\
    & \xrightarrow{p} E(Y_1) + E\left[\left(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})} \right) (m(\mathbf{W}_1, \hat\gamma)-Y_1)\right] \text{ (WLLN)} \\
    & \phantom{\xrightarrow{p} E(Y_1)} = \beta + E\left[\left(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})} \right) (m(\mathbf{W}_1, \hat\gamma)-Y_1)\right]
\end{align*}

So the DR estimator is a consistent estimator for the true mean $\beta$ if the expectation added to $\beta$ above is equal to 0.

\textbf{Case 1:} Suppose the propensity score $\pi(\mathbf{W},\hat{\alpha})$ is correctly specified and $\hat{\alpha} \xrightarrow{p} \alpha$, therefore  $\pi(\mathbf{W},\hat{\alpha}) \xrightarrow{p} \pi(\mathbf{W}) = E(R|\mathbf{W}) = E(R|\mathbf{W}, Y)$ under MAR assumption. Then using the tower rule on expectations, we have
\begin{align*}
     & E \left[\left(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})} \right) (m(\mathbf{W}_1, \hat\gamma)-Y_1)\right]  \\
     & \phantom{E [(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})})} = E\left[E\left[\left(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})} \right) (m(\mathbf{W}_1, \hat\gamma)-Y_1)|Y_1, \mathbf{W}_1\right]\right] \\
     & \phantom{E [(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})})} = E\left[ (m(\mathbf{W}_1, \hat\gamma)-Y_1)E\left[\left(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})} \right)|Y_1, \mathbf{W}_1\right]\right] \\
     & \phantom{E [(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})})} = E\left[ (m(\mathbf{W}_1, \hat\gamma)-Y_1)\left(1 - \frac{E(R_1|Y_1, \mathbf{W}_1)}{\pi(\mathbf{W_1},\hat{\alpha})} \right)\right] \\
     & \phantom{E [(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})})} = E\left[ (m(\mathbf{W}_1, \hat\gamma)-Y_1)\left(1 - \frac{E(R_1|Y_1, \mathbf{W}_1)}{E(R_1|Y_1, \mathbf{W}_1)} \right)\right] \\
     & \phantom{E [(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})})} = 0
\end{align*}
Hence $\hat{\beta}_{DR} \xrightarrow{p} \beta$.

\textbf{Case 2:} Suppose the outcome model $m(\mathbf{W}, \hat\gamma)$ is correctly specified and $\hat{\gamma} \xrightarrow{p} \gamma$, therefore  $m(\mathbf{W}, \hat\gamma) \xrightarrow{p} m(\mathbf{W}) = E(Y|\mathbf{W}) = E(Y|\mathbf{W}, R)$ under MAR assumption. Then a similar manner to case 1, we have
\begin{align*}
     & E\left[\left(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})} \right) (m(\mathbf{W}_1, \hat\gamma)-Y_1)\right] \\
     & \phantom{E [(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})})} = E\left[ E\left[\left(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})} \right)(m(\mathbf{W}_1, \hat\gamma)-Y_1)|R_1, \mathbf{W}_1\right]\right] \\
     & \phantom{E [(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})})} = E\left[\left(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})} \right) E\left[(m(\mathbf{W}_1, \hat\gamma)-Y_1)|R_1, \mathbf{W}_1\right]\right] \\
     & \phantom{E [(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})})} = E\left[\left(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})} \right) (m(\mathbf{W}_1, \hat\gamma)-E\left[Y_1|R_1, \mathbf{W}_1\right])\right] \\
     & \phantom{E [(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})})} = E\left[\left(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})} \right) (E\left[Y_1|R_1, \mathbf{W}_1\right])-E\left[Y_1|R_1, \mathbf{W}_1\right])\right] \\
     & \phantom{E [(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})})} = 0
\end{align*}
Hence $\hat{\beta}_{DR} \xrightarrow{p} \beta$.

Thus we have shown that when at least one of the two models is correctly specified, the DR estimator is consistent for the true mean. It follows naturally that if both models are correctly specified, this result still holds.
\subsection{Parametric DR estimator}

In standard parametric DR estimation, it is fairly common to define the propensity and outcome models as logistic or linear regression models. These methods are user-friendly and fairly easy to understand and compute, as many pre-made functions are available in coding languages.\\

Although this version of the DR estimator that these models lead to has been established as the usual estimator, \citeauthor{kang} have shown it performs poorly when either the propensity or both models are misspecified, i.e when logistic and linear regressions aren't the judicious choice of models for the data. This gives no guarantee that $\hat\beta_{DR}$ is at least as efficient as the IPW estimator, which we were seeking to improve on.

Another issue with parametric DR estimators is that $\hat\beta_{DR}$ may be outside the range of the observed $\mathbf{Y}$ values; it could lie below or above the minimum or maximum value of $Y$, which would not be useful. This is especially true when we may have binary outcome (\citeauthor{vansteelandt}).

\subsection{Simulation study of parametric DR}
\label{para_model}

Let us look at an example to illustrate the use of parametric DR estimators. We take the model specification from \citet{benkeser2017}: our co-variate vector is $\mathbf{W} = (W_1, W_2)$, where $W_1$ is uniformly distributed over $[-2,2]$ and $W_2$ has a Bernoulli distribution of success probability $1/2$. $W_1$ and $W_2$ are independent. The missingness indicator $R$ is a Bernoulli random variable of success probability $\expit(-w_1 + 2w_1w_2)$. The true propensity score is therefore
\begin{align*}
    \pi(\mathbf{W}) = P(R = 1 |\mathbf{W} = (w_1,w_2)) = \expit(-w_1 + 2w_1w_2)
\end{align*}

The partially observed outcome has a conditional probability of occurrence 
\begin{align*}
    P(Y = 1|R = r,\mathbf{W} = (w_1, w_2)) = \expit(0.2r - w_1 + 2w_1w_2)
\end{align*}

The true outcome model $m(W)$ is then 
\begin{align*}
    E(Y|\mathbf{W}, R=1) &= P(Y = 1|\mathbf{W}, R= 1) \\
    & = \expit(0.2 - w_1 + 2w_1w_2)
\end{align*}

From there we calculated four parametric DR estimates: 
\begin{itemize}
    \item \texttt{DR logistic}, where the propensity and outcome models are correctly specified by logistic regression fits that account for the interaction term between $W_1, W_2$;
    \item \texttt{DR right}, where both models are correctly specified using the exact formulas above;
    \item \texttt{DR wrong outcome model}, where the outcome model is incorrectly specified by not accounting for the interaction term between $W_1, W_2$ in the logistic regression fit;
    \item \texttt{DR wrong propensity score}, where the propensity model is incorrectly specified by not accounting for the interaction term between $W_1, W_2$ in the logistic regression fit.
\end{itemize}

For each estimates we considered sample sizes $N = 200, 400, 600, ..., 2000$ and for each N we calculated the empirical bias of the estimators over 2500 generated data sets, as well as the empirical coverage of the 95\% confidence intervals and accuracy of the standard error estimation from \citet{lunceford_davidian}.

To calculate the bias we must have the true expectation of $Y$, which is equal to taking the expectation of $Y$ given $R = 1$:

\begin{align*}
    E(Y|R = 1) &= P(Y = 1|R = 1)\\
    &= \int_{-2}^{2} \sum_{w_2 = 0}^{1} P(Y = 1, W_1 = w_1, W_2 = w_2|R = 1) dw_1 \\
    & = \int_{-2}^{2} \sum_{w_2 = 0}^{1} P(Y = 1|W_1 = w_1, W_2 = w_2, R = 1) \\
    & \phantom{ = \int_{-2}^{2} \sum_{w_2 = 0}^{1}} \cdot P(R = 1, W_1 = w_1, W_2 = w_2)dw_1 \\
    & = \int_{-2}^{2} \sum_{w_2 = 0}^{1} \expit(0.2 - w_1 + 2w_1w_2)P(R = 1|W_1 = w_1, W_2 = w_2) \\
    &\phantom{ = \int_{-2}^{2} \sum_{w_2 = 0}^{1}} \cdot P(W_1 = w_1)P(W_2 = w_2)dw_1 \\
    & = \int_{-2}^{2} \sum_{w_2 = 0}^{1} \expit(0.2 - w_1 + 2w_1w_2)\expit(-w_1+2w_1w_2) \cdot \frac{1}{4}\cdot \frac{1}{2}dw_1 \\
    & = \int_{-2}^{2} \frac{1}{8}[\expit(0.2 - w_1)\expit(-w_1) \\
    &\phantom{ = \int_{-2}^{2} \sum_{w_2 = 0}^{1}} +\expit(0.2 - w_1 + 2w_1)\expit(-w_1+2w_1)]dw_1 \\
    & = \int_{-2}^{2} \frac{1}{8}[\expit(0.2 - w_1)\expit(-w_1) +\expit(0.2 + w_1)\expit(w_1)]dw_1 \\
\end{align*}

By calculating the above integral numerically, we get that the true mean, had all outcome variables $Y_i$ been accounted for, is about 0.53.

For calculating the confidence intervals, we set the model-based sample error estimates to be 
\begin{align*}
    \hat{SE} = n^{-1} \sqrt{\sum_{i=1}^n \left(\frac{R_i(Y_i - m(\mathbf{W}_i))}{\pi(\mathbf{W}_i)} + m(\mathbf{W}_i) - \hat{\beta}_{DR}\right)^2}
\end{align*}
from \citet{lunceford_davidian}. An issue with this estimate of the standard error is that it is specifically for DR estimates with correctly specified nuisance parameters; we will cover the issue with one incorrectly specified nuisance parameter later.

\begin{itemize}
    \item Figure \ref{figbiaspara} suggests that the DR estimators are indeed consistent as the biases converge to 0 as the sample sizes $N$ get larger. We also notice that the DR estimation with wrong propensity score has the poorest convergence out of the 4 estimations, which is in line with the inefficiency that was suggested for such specifications by \citet{kang}.
    \item Figure \ref{figCIpara} shows the empirical coverage of the 95\% confidence intervals of each estimations, which indeed converge to 95\% for the correctly specified DR estimations. Because the estimation of the standard error was specifically for correctly specified model, the coverage for the DR estimations with one wrong nuisance parameter are either too high or too low.
    \item Figure \ref{figSEpara} confirms the statement above, as we see the ratio of the standard deviation of the DR estimates to the average standard error estimator is close to 1 for correctly specified models, hence suggesting a correct calculation of the confidence interval bounds. As for the DR estimations with one wrong model, this ratio is above or below 1, suggesting that the standard error estimation is on average too low or too high, hence giving a CI coverage that is too low or too high for these cases as the calculations of the CI bounds are inaccurate.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/biaspara.png}
    \caption{Simulation results of bias for the various DR estimator specifications}
    \label{figbiaspara}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/CIpara.png}
    \caption{Simulation results of 95\% confidence intervals coverage for the various DR estimator specifications}
    \label{figCIpara}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/SEpara.png}
    \caption{Accuracy of \citet{lunceford_davidian} SE estimation for the various DR estimator specifications}
    \label{figSEpara}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/sqrtnpara.png}
    \caption{Simulation results of $\sqrt{N}$bias for the various DR estimator specifications}
    \label{figsqrtnpara}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/histpara.png}
    \caption{Histogram of simulation results for  sample size $N = 2000$}
    \label{fighistpara}
\end{figure}

Although these results suggest that the parametric methods for setting the nuisance parameters is a good way of specifying these models, as shown by the consistency of the estimates, in reality we were only able to set the models correctly because we knew the distribution of the data generative method, and therefore the fact that there was an interactive term between $W_1,W_2$ in the logistic regression model. Had we been given this data set without any information on how it was generated, it would have been a lot more time-consuming to find the correct specification of at least one nuisance parameter. This is where the practicality of ensemble learners from Machine Learning tools seem to be a more efficient way of setting these models, which we will cover in the next section. Nevertheless, a important advantage of parametric DR estimation is the user-friendly aspect, as we have mentioned previously that logistic or linear regressions models are easy fitting methods to start with.

\section{Machine Learning approaches to optimise the DR estimator}

Parametric models for the propensity score and outcome regression give more certainty, thanks to their explicit formulae and theoretical backings, but we have seen in the simulation study above that even a slight misspecification can lead to biased estimations. However we can apply asymptotic results such as the central limit theorem and the law of large numbers for convenient asymptotic normality and confidence intervals (CI).

But convenience is over weighed by defectiveness under model misspecification as we mentioned, so the probability of the CI containing the true mean converges to 0 \citep{diaz}.

Alternatively, one can use machine learning models to try to alleviate this bias, as they often provide models that are data adaptive and therefore easier to get a correctly specified model. In fact, \citet{ps_SL} have shown that when the propensity score is incorrectly specified, the use of Machine Learning tools for specifying the outcome regression can still give an unbiased DR estimator, which is an advantage over the parametric scenario, as we had seen that an incorrect propensity score with correct outcome regression was still giving the worse DR estimator in terms of bias in section \ref{para_model}.

Some data adaptive methods include: random forests, support vector machines, kernel regression, outcome weighed learning, Q-learning, neural networks, ensembles of these methods, etc. However, these methods may not have quantifiable theoretical guarantees. In the following sections we will take a closer look at the use of Random Forests, Support Vector Machines and Multi-layer Perceptron with various data settings, and see if these data adaptive methods give better estimates compared to the results from parametric models, even accounting for higher model complexity by adding more covariates to the data.

\subsection{Using Random Forests}

Random forests are a beginner friendly Machine Learning tool that create a classification based on multiple randomly generated decision trees that are combined together to make a better prediction than that of a single decision tree. It is easy to use and fit thanks to pre-existing packages in Python and R, and they are also comfortable working with a mix of continuous and categorical features. However it is not the most time efficient tool, as the training of the models requires a lot of computational power.

\subsubsection{Simulation 1: 2 features}

We use the same data generated in section \ref{para_model}, and this time we calculate DR estimators with various nuisance parameter settings that use logistic regression or random forests:
\begin{itemize}
    \item \texttt{DR logistic ps, forest om}: the propensity is a correctly specified logistic regression, and the outcome model is a random forest. Both model account for the interaction term between $W_1$ and $W_2$
    \item \texttt{DR logistic om, forest ps}: the outcome model is a correctly specified logistic regression, and the propensity score is a random forest.
    \item \texttt{DR forest}: both the propensity and the outcome model are random forests that account for the interaction term between $W_1$ and $W_2$.
    \item \texttt{DR wrong logistic om, forest ps}: the outcome model is an incorrectly specified logistic regression, and the propensity score is a correctly specified random forest.
    \item \texttt{DR wrong logistic ps, forest om}: the propensity is an incorrectly specified logistic regression, and the outcome model is a correctly specified random forest.
    \item \texttt{DR wrong forest om, forest ps}: the outcome model is an incorrectly specified random forest (i.e no interaction term), and the propensity score is a correctly specified random forest.
    \item \texttt{DR wrong forest ps, forest om}: the propensity is an incorrectly specified random forest (i.e no interaction term), and the outcome model is a correctly specified random forest.
    \item \texttt{DR wrong forest ps \& om}: both the propensity and the outcome model are incorrectly specified random forests.
    \item \texttt{DR exact}: both the propensity and outcome model are exactly specified by using the distributions from the data generative methods.
\end{itemize}

Each time a random forest is specified, it goes through a stratified 5-fold cross validation of hyper parameters in order to input the parameters that give a forest with the best accuracy on the test sets. 

As before, we obtain an empirical bias by taking the sample mean of 2500 generated estimators of each sample size varying from 200 to 1200 rows.

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/biasRF.png}
    \caption{Simulation results of bias for the various DR estimator specifications}
    \label{figbiasRF}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/sqrtnRF.png}
    \caption{Accuracy of \citet{lunceford_davidian} SE estimation for the various DR estimator specifications}
    \label{figsqrtnRF}
\end{figure}

We notice in figure \ref{figbiasRF} that the models with random forests still provide DR estimates with converging bias, even when the propensity and outcome model are set using a forest that doesn't account for the interaction term, suggesting that the forest is able to recover the misspecification, making it a useful tool in setting the nuisance parameters. 

We also notice that when the outcome model was a forest and the propensity was wrongly specified with a model that doesn't account for the interaction term, whether that was a logistic regression or a Random Forest, the bias still performs well, which is in line with what \citet{ps_SL} have shown about the recovery of unbiasedness with ML tools for the outcome model when dealing with a wrong propensity score. 

However this figure shows an unusual result for the model that had a wrong logistic outcome model, paired with a forest propensity score;  whereas when the misspecification of the outcome model is through a forest, the bias still performs well (its brown line on the graph is overlapped with the grey line). This makes us wonder if a forest on the propensity is not strong enough to recover the parametric misspecification of the outcome model.\\

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/CIRF.png}
    \caption{Simulation results of 95\% confidence intervals coverage for the various DR estimator specifications}
    \label{figCIRF}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/SERF.png}
    \caption{Accuracy of \citet{lunceford_davidian} SE estimation for the various DR estimator specifications}
    \label{figSERF}
\end{figure}



As for the CI coverage results of the various DR estimations in figure \ref{figCIRF}, we notice that their convergence is slower compared to the parametric setting in section \ref{para_model}, and perhaps getting results from larger sample sizes would have shown that they eventually reach the 95\% goal. Figure \ref{figSERF} shows that the standard deviation of the estimated means also converge to the estimated value of the standard error at a slower rate than the parametric setting, hence explaining the slowness in CI coverage convergence as well. However for the DR estimator with the wrong logistic outcome model and forest propensity, this CI coverage simply converges to 0(with the standard error estimate diverging from the standard deviation in \ref{figSERF}), so we eventually don't contain the true value of the sample mean in the confidence intervals based on this specification of the DR estimator. Again, more exploration of DR estimations using a wrong parametric outcome model and a random forest propensity is needed to see why this phenomenon is happening.

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/histRF.png}
    \caption{Accuracy of \citet{lunceford_davidian} SE estimation for the various DR estimator specifications}
    \label{fighistRF}
\end{figure}

\subsubsection{Simulation 2: 4 features}
\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/biasRF.png}
    \caption{Simulation results of bias for the various DR estimator specifications}
    \label{figbiasRF}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/sqrtnRF_moreW.png}
    \caption{Accuracy of \citet{lunceford_davidian} SE estimation for the various DR estimator specifications}
    \label{figsqrtnRF}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/CIRF.png}
    \caption{Simulation results of 95\% confidence intervals coverage for the various DR estimator specifications}
    \label{figCIRF}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/SERF.png}
    \caption{Accuracy of \citet{lunceford_davidian} SE estimation for the various DR estimator specifications}
    \label{figSERF}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/histRF.png}
    \caption{Accuracy of \citet{lunceford_davidian} SE estimation for the various DR estimator specifications}
    \label{fighistRF}
\end{figure}
\subsection{Using Support Vector Machines}

\subsection{Using Multi-Layer Perceptrons}

This section is motivated by the publication by \citet{setoguchi-nn} suggesting that Neura Networks reduce the bias of propensity scores compared to logistic regression.
\subsection{Comparison of the three methods}

\section{Discussion}

\section{Conclusion}

\section{Appendix}
%% bibliography
\clearpage
\bibliographystyle{unsrtnat}
\bibliography{bibliography}


\end{document}
