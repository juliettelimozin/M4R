\documentclass[12pt,twoside]{article}
\usepackage{indentfirst}
\usepackage[nottoc]{tocbibind}
\usepackage{fancyhdr,ragged2e}
\usepackage{todonotes}
\fancyhead{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Definitions for the title page
% Edit these to provide the correct information
% e.g. \newcommand{\reportauthor}{Timothy Kimber}

\newcommand{\reporttitle}{Evaluating the Use of Machine Learning for Doubly-Robust Estimation in Missing Data}
\newcommand{\reportauthor}{Juliette Maiko Limozin}
\newcommand{\supervisor}{Dr David Whitney}
\newcommand{\degreetype}{Mathematics}
\newcommand{\expit}{\text{expit}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% load some definitions and default packages
\input{includes}

% load some macros
\input{notation}

\date{October 2020}

\begin{document}

% load title page
\input{titlepage}


% page numbering etc.
\pagenumbering{roman}
\clearpage{\pagestyle{empty}\cleardoublepage}
\setcounter{page}{1}
\pagestyle{fancy}
\setlength{\parindent}{5ex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Your abstract.cddvfvfbb


\end{abstract}

\cleardoublepage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgments}
I would like to thank my project supervisor, Dr David Whitney, for his continued guidance and support throughout this project. Our weekly meetings have always been a pleasure, and his advice on postgraduate studies have also been of immense help. 

I would also like to thank my family, who even though might not understand Statistics at a Master's level, have always enabled me to achieve my best.

Last but not least, I would like to thank my various statistics lecturers who, in my 4 years at Imperial, have enamoured me with their passion for Statistics.

\clearpage{\pagestyle{empty}\cleardoublepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--- table of contents
\tableofcontents 


\clearpage{\pagestyle{empty}\cleardoublepage}
\pagenumbering{arabic}
\setcounter{page}{1}
\fancyhead[L]{\textsl{\leftmark}}

\setcitestyle{authoryear,open={(},close={)}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction} 
\todo[inline]{Finish introduction}

- Clinical trials often have reserve patients because the trial patients might drop out for x and y reason

- often the calculation of the mean outcome of the trial is needed, but what do we do with the patients we don't have an outcome for?

- in this report we will cover the Doubly-Robust mean estimation method combined with machine learning tools that will help generate a consistent estimation of the mean thanks to easy to use computational tools

- Hopefully this report can be useful to upcoming clinical trials in making them understand how the machine learning models can be use to improve mean estimation from missing data

\section{Setting} 

Suppose we have a data set of size $n$, and realisations of random vector $(\mathbf{Y}, \mathbf{W}, \mathbf{R})$ where $\mathbf{Y}$ denotes a partially denoted outcome, i.e. $Y_i$ is missing for individual $i$'s unobserved outcome. $\mathbf{W}$ denotes the auxiliary variables, and $\mathbf{R}$ is an indicator on whether the $i$th individual's outcome is missing or not; that is, if $R_i = 1$, the outcome is observed, whereas $R_i = 0$ would indicate that the outcome is missing.
To summarise:
\begin{itemize}
    \item $\mathbf{Y}$: partially observed outcome 
    \item $\mathbf{W}$: auxiliary variables/covariates 
    \item $\mathbf{R}$: indicator on whether $Y$ is observed or not ($R = 1$: observed, $R = 0$: missing) 
\end{itemize}

A data set is said to be \textbf{missing at random (MAR)} if the conditional probability that a particular missingness pattern occurs given the data is independent of the missing values in that pattern \citep{vansteelandt}, i.e $P(R=r|\mathbf{W}, Y) = P(R= r|\mathbf{W})$ and  $P(Y=y|\mathbf{W}, R) = P(Y=y|\mathbf{W})$. So in the example of a clinical trial, we would consider the data set obtained from the study to be MAR if given the measurements taken from the patients, the reasons why some patients dropped out were unrelated to the outcome of the trial. The MAR setting will therefore have effects on any causal inference we may perform on this data set.

When the missingness pattern is independent of both the auxiliary variables and the outcome data, the data set is said to be \textbf{missing completely at random (MCAR)}.\\

We make the assumption that $(W_1, Y_1, R_1),...,(W_n, Y_n, R_n)$ are independent and identically distributed for the rest of the report. We also assume that $R$ is independent of $Y$ given W so $(\mathbf{W}_1, Y_1, R_1), ... ,(\mathbf{W}_n, Y_n, R_n)$ are MAR (\citeauthor{vansteelandt}), where the index $i = 1,...,n$ denotes the $i$th individual of the data.\\

\section{Inverse probability weighting and doubly robust estimators}

\subsection{Inverse Probability Weighting approach}

Suppose we wish to calculate or estimate the outcome mean $\beta = E(\mathbf{Y})$. A problem occurs when trying to compute it as $\mathbf{Y}$ is only partially observed, so calculating a mean with only the observed outcomes would not be a good representation of the data outcome as a whole.

One solution to this problem is the Inverse Probability Weighting (IPW) approach. Its name comes from the fact that each observed outcome $Y_i$ (called \textit{complete} case) is weighted by the probability that the $i$th individual is observed given the set of variables for this outcome, $\mathbf{W}_i$ (\citeauthor{vansteelandt}). We denote this weight by
\begin{align*}
    \pi(\mathbf{W})= E(R|\mathbf{W}) = P(R = 1|\mathbf{W}).
\end{align*}

We assume that the weighting is positive for any $\mathbf{W}_i$.

The IPW estimator of the mean $\beta$ is then simply 
\begin{equation} \label{IPW_est}
    \hat{\beta}_{IPW} = \frac{1}{n} \sum_{i=1}^{n} \frac{R_iY_i}{\pi(\mathbf{W}_i)}.
\end{equation}

As $\pi(\mathbf{W})$ is unknown as the data is missing at random, we propose a propensity model, most commonly a parametric model $\pi(\mathbf{W}; \boldsymbol\alpha)$ for the missingness pattern and we build and estimator $\hat{\boldsymbol\alpha}$ of $\boldsymbol\alpha$ from the data (\citeauthor{vansteelandt}).

The IPW estimator is consistent for $\beta$ if the propensity model $\pi(\mathbf{W}; \boldsymbol\alpha)$ is correctly specified \citep{davidian}. 

If the propensity model $\pi(\mathbf{W})$ is correctly specified, then by the weak law of large numbers (WLLN), $\hat{\beta}_{IPW}$ will converges in probability to the true mean $\beta$, so this makes the IPW estimate consistent for $\beta$. Using the Central Limit Theorem (CLT), we have $\sqrt{n}(\hat{\beta}_{IPW}-\beta) \xrightarrow{d} N(0,\sigma_1^2)$ where $\sigma$ is the estimated outcome variance, so we can use this result to perform inference tests and confidence intervals for the true mean.

A quick drawback from the IPW estimator in \ref{IPW_est} is that it is usually difficult to construct at propensity model unless the auxiliary variables $\mathbf{W}$ are categorical or the sample size $n$ is large enough, so a removal of bias is not guaranteed by the IPW estimator when he model is misspecified. Another problem that arises is unstable weights: if the model is misspecified, the fitted probability of observing an outcome given the auxiliary variables may be very small for the majority of individuals, and very large for a select few, so the IPW estimator is mainly dominated by these large weights \citep{seaman}.

\citet{kang} suggest that the model being misspecified is more likely the cause of large weights, rather than if the auxiliary variables are truly predictive of whether the outcome may be missing or not. There exist a few tests, including the \citet{hosmer} test, to detect poor fit of the propensity model.\\

\subsection{Regression imputation approach} 

Alternatively, one could make use of the conditional expectation of the outcome 
\begin{align*}
    m(\mathbf{W}) = E(Y|\mathbf{W})
\end{align*}
to have a different estimator of the mean \citep{davidian,vansteelandt}, sometimes called the regression imputation estimator: 

\begin{equation}
    \hat{\beta}_ {RI} = \frac{1}{n}\sum_{i = 1}^n m(\mathbf{W}_i)
\end{equation}

Because again the true value of $m(\mathbf{W})$ is unknown, we propose an outcome model, most commonly a parametric model $m(\mathbf{W}, \gamma)$ with some parameter $\gamma$, and again we build an estimator $\hat{\gamma}$ for $\gamma$ from the sample data. Again, like in IPW, the efficiency of the the RI estimator relies on the correct specification of the outcome model, as then the WLLN would apply and so the RI estimator would be consistent for $\beta$. \\

\subsection{Augmented IPW approach} 

However this motivates the idea behind the augmented IPW estimator. One could think of combining the ideas from the IPW and RI estimators. From \citet{davidian}, estimators of $\beta$, when the propensity model is correctly specified and the estimators are consistent and are asymptotically normal(i.e. CLT holds), are equivalent to 

\begin{equation}
    \frac{1}{n}\sum_{i=1}^{n}\frac{R_iY_i}{\pi(\mathbf{W}_i, \hat{\alpha})} - \frac{1}{n}\sum_{i=1}^{n} \left(1 - \frac{R_i}{\pi(\mathbf{W_i},\hat{\alpha})} \right) h(\mathbf{W}_i)
\end{equation}

This is called an augmented IPW as it is the sum of the IPW estimator and an augmentation term depending on $h(\mathbf{W})$ \citep{davidian}. By taking $-h(\mathbf{W}) = m(\mathbf{W}, \hat{\gamma})$ like in the RI approach, we have a new estimator
\begin{equation}
    \hat\beta_{DR} = \frac{1}{n}\sum_{i=1}^{n}\frac{R_iY_i}{\pi(\mathbf{W}_i, \hat{\alpha})} + \frac{1}{n}\sum_{i=1}^{n} \left(1 - \frac{R_i}{\pi(\mathbf{W_i},\hat{\alpha})} \right) m(\mathbf{W}_i, \hat\gamma)
\end{equation}

$\hat\beta_{DR}$ is consistent (i.e $\hat\beta_{DR}$ converges in probability to the true mean $\beta$) and asymptotically normal when either the propensity score or outcome model are correctly specified, as the correct specification of one suffices for consistency, and therefore the CLT to hold for $\hat\beta_{DR}$, which is why we call this a \textbf{doubly-robust estimator} for $\beta$. This allows us to not require a correct specification for the entire outcome ans missingness distributions whilst maintaining a better efficiency than IPW estimators \citep{bangrobins,vansteelandt}.

For example, doubly-robust estimators are used to estimate the mean of clinical trial outcome when there have been dropouts mid-trial; we use acquired variables/observations on patients to compute a DR estimation of the mean. \\

The proof of the doubly-robustness of this mean estimation is as follows, mainly taken from \citet{vansteelandt}.

Rewriting $\hat{\beta}_{DR}$, we use the WLLN to get
\begin{align*}
    \hat{\beta}_{DR} & = \frac{1}{n}\sum_{i=1}^{n}\frac{R_iY_i}{\pi(\mathbf{W}_i, \hat{\alpha})} + \frac{1}{n}\sum_{i=1}^{n} \left(1 - \frac{R_i}{\pi(\mathbf{W_i},\hat{\alpha})} \right) m(\mathbf{W}_i, \hat\gamma) \\
    & = \frac{1}{n}\sum_{i=1}^{n} Y_i + \frac{1}{n}\sum_{i=1}^{n}\left(1 - \frac{R_i}{\pi(\mathbf{W_i},\hat{\alpha})} \right) (m(\mathbf{W}_i, \hat\gamma)-Y_i) \\
    & \xrightarrow{p} E(Y_1) + E\left[\left(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})} \right) (m(\mathbf{W}_1, \hat\gamma)-Y_1)\right] \text{ (WLLN)} \\
    & \phantom{\xrightarrow{p} E(Y_1)} = \beta + E\left[\left(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})} \right) (m(\mathbf{W}_1, \hat\gamma)-Y_1)\right]
\end{align*}

So the DR estimator is a consistent estimator for the true mean $\beta$ if the expectation added to $\beta$ above is equal to 0. \\

\textbf{Case 1:} Suppose the propensity score $\pi(\mathbf{W},\hat{\alpha})$ is correctly specified and $\hat{\alpha} \xrightarrow{p} \alpha$, therefore  $\pi(\mathbf{W},\hat{\alpha}) \xrightarrow{p} \pi(\mathbf{W}) = E(R|\mathbf{W}) = E(R|\mathbf{W}, Y)$ under MAR assumption. Then using the tower rule on expectations, we have
\begin{align*}
     & E \left[\left(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})} \right) (m(\mathbf{W}_1, \hat\gamma)-Y_1)\right]  \\
     & \phantom{E [(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})})} = E\left[E\left[\left(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})} \right) (m(\mathbf{W}_1, \hat\gamma)-Y_1)|Y_1, \mathbf{W}_1\right]\right] \\
     & \phantom{E [(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})})} = E\left[ (m(\mathbf{W}_1, \hat\gamma)-Y_1)E\left[\left(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})} \right)|Y_1, \mathbf{W}_1\right]\right] \\
     & \phantom{E [(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})})} = E\left[ (m(\mathbf{W}_1, \hat\gamma)-Y_1)\left(1 - \frac{E(R_1|Y_1, \mathbf{W}_1)}{\pi(\mathbf{W_1},\hat{\alpha})} \right)\right] \\
     & \phantom{E [(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})})} = E\left[ (m(\mathbf{W}_1, \hat\gamma)-Y_1)\left(1 - \frac{E(R_1|Y_1, \mathbf{W}_1)}{E(R_1|Y_1, \mathbf{W}_1)} \right)\right] \\
     & \phantom{E [(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})})} = 0
\end{align*}
Hence $\hat{\beta}_{DR} \xrightarrow{p} \beta$. \\

\textbf{Case 2:} Suppose the outcome model $m(\mathbf{W}, \hat\gamma)$ is correctly specified and $\hat{\gamma} \xrightarrow{p} \gamma$, therefore  $m(\mathbf{W}, \hat\gamma) \xrightarrow{p} m(\mathbf{W}) = E(Y|\mathbf{W}) = E(Y|\mathbf{W}, R)$ under MAR assumption. Then a similar manner to case 1, we have
\begin{align*}
     & E\left[\left(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})} \right) (m(\mathbf{W}_1, \hat\gamma)-Y_1)\right] \\
     & \phantom{E [(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})})} = E\left[ E\left[\left(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})} \right)(m(\mathbf{W}_1, \hat\gamma)-Y_1)|R_1, \mathbf{W}_1\right]\right] \\
     & \phantom{E [(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})})} = E\left[\left(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})} \right) E\left[(m(\mathbf{W}_1, \hat\gamma)-Y_1)|R_1, \mathbf{W}_1\right]\right] \\
     & \phantom{E [(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})})} = E\left[\left(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})} \right) (m(\mathbf{W}_1, \hat\gamma)-E\left[Y_1|R_1, \mathbf{W}_1\right])\right] \\
     & \phantom{E [(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})})} = E\left[\left(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})} \right) (E\left[Y_1|R_1, \mathbf{W}_1\right])-E\left[Y_1|R_1, \mathbf{W}_1\right])\right] \\
     & \phantom{E [(1 - \frac{R_1}{\pi(\mathbf{W_1},\hat{\alpha})})} = 0
\end{align*}
Hence $\hat{\beta}_{DR} \xrightarrow{p} \beta$.\\

Thus we have shown that when at least one of the two models is correctly specified, the DR estimator is consistent for the true mean. It follows naturally that if both models are correctly specified, this result still holds.
\subsection{Parametric DR estimation}

In standard parametric DR estimation, it is fairly common to define the propensity and outcome models as logistic or linear regression models. These methods are user-friendly and fairly easy to understand and compute, as many pre-made functions are available in coding languages. They also offer theoretical backing that would allow us to make statistical inference.\\

Although this version of the DR estimator that these models lead to has been established as the usual estimator, \citeauthor{kang} have shown it performs poorly when either the propensity or both models are misspecified, i.e when logistic and linear regressions aren't the judicious choice of models for the data. This gives no guarantee that $\hat\beta_{DR}$ is at least as efficient as the IPW estimator, which we were seeking to improve on.

Another issue with parametric DR estimators is that $\hat\beta_{DR}$ may be outside the range of the observed $\mathbf{Y}$ values; it could lie below or above the minimum or maximum value of $Y$, which would not be useful. This is especially true when we may have binary outcome (\citeauthor{vansteelandt}).

\subsection{Simulation study of parametric DR}
\label{para_model}

Let us look at an example to illustrate the use of parametric DR estimators. We take the model specification from \citet{benkeser2017}: our co-variate vector is $\mathbf{W} = (W_1, W_2)$, where $W_1$ is uniformly distributed over $[-2,2]$ and $W_2$ has a Bernoulli distribution of success probability $1/2$. $W_1$ and $W_2$ are independent. The missingness indicator $R$ is a Bernoulli random variable of success probability $\expit(-w_1 + 2w_1w_2)$. The true propensity score is therefore
\begin{align*}
    \pi(\mathbf{W}) = P(R = 1 |\mathbf{W} = (w_1,w_2)) = \expit(-w_1 + 2w_1w_2)
\end{align*}

The partially observed outcome has a conditional probability of occurrence 
\begin{align*}
    P(Y = 1|R = r,\mathbf{W} = (w_1, w_2)) = \expit(0.2r - w_1 + 2w_1w_2)
\end{align*}

The true outcome model $m(\mathbf{W})$ is then 
\begin{align*}
    E(Y|\mathbf{W}, R=1) &= P(Y = 1|\mathbf{W}, R= 1) \\
    & = \expit(0.2 - w_1 + 2w_1w_2)
\end{align*}

From there we calculated four parametric DR estimates: 
\begin{itemize}
    \item \texttt{DR logistic}, where the propensity and outcome models are correctly specified by logistic regression fits that account for the interaction term between $W_1, W_2$;
    \item \texttt{DR right}, where both models are correctly specified using the exact formulas above;
    \item \texttt{DR wrong outcome model}, where the outcome model is incorrectly specified by not accounting for the interaction term between $W_1, W_2$ in the logistic regression fit;
    \item \texttt{DR wrong propensity score}, where the propensity model is incorrectly specified by not accounting for the interaction term between $W_1, W_2$ in the logistic regression fit.
\end{itemize}

For each estimates we considered sample sizes $N = 200, 400, 600, ..., 2000$ and for each N we calculated the empirical bias of the estimators over 2500 generated data sets, as well as the empirical coverage of the 95\% confidence intervals and accuracy of the standard error estimation from \citet{lunceford_davidian}.

To calculate the bias we must have the true expectation of $Y$, which is equal to taking the expectation of $Y$ given $R = 1$:

\begin{align*}
    E(Y|R = 1) &= P(Y = 1|R = 1)\\
    &= \int_{-2}^{2} \sum_{w_2 = 0}^{1} P(Y = 1, W_1 = w_1, W_2 = w_2|R = 1) dw_1 \\
    & = \int_{-2}^{2} \sum_{w_2 = 0}^{1} P(Y = 1|W_1 = w_1, W_2 = w_2, R = 1) \\
    & \phantom{ = \int_{-2}^{2} \sum_{w_2 = 0}^{1}} \cdot P(R = 1, W_1 = w_1, W_2 = w_2)dw_1 \\
    & = \int_{-2}^{2} \sum_{w_2 = 0}^{1} \expit(0.2 - w_1 + 2w_1w_2)P(R = 1|W_1 = w_1, W_2 = w_2) \\
    &\phantom{ = \int_{-2}^{2} \sum_{w_2 = 0}^{1}} \cdot P(W_1 = w_1)P(W_2 = w_2)dw_1 \\
    & = \int_{-2}^{2} \sum_{w_2 = 0}^{1} \expit(0.2 - w_1 + 2w_1w_2)\expit(-w_1+2w_1w_2) \cdot \frac{1}{4}\cdot \frac{1}{2}dw_1 \\
    & = \int_{-2}^{2} \frac{1}{8}[\expit(0.2 - w_1)\expit(-w_1) \\
    &\phantom{ = \int_{-2}^{2} \sum_{w_2 = 0}^{1}} +\expit(0.2 - w_1 + 2w_1)\expit(-w_1+2w_1)]dw_1 \\
    & = \int_{-2}^{2} \frac{1}{8}[\expit(0.2 - w_1)\expit(-w_1) +\expit(0.2 + w_1)\expit(w_1)]dw_1 \\
\end{align*}

By calculating the above integral numerically, we get that the true mean, had all outcome variables $Y_i$ been accounted for, is about 0.53.

For calculating the confidence intervals, we set the model-based sample error estimates to be 
\begin{align*}
    \hat{SE} = n^{-1} \sqrt{\sum_{i=1}^n \left(\frac{R_i(Y_i - m(\mathbf{W}_i))}{\pi(\mathbf{W}_i)} + m(\mathbf{W}_i) - \hat{\beta}_{DR}\right)^2}
\end{align*}
from \citet{lunceford_davidian}.

\begin{itemize}
    \item Figure \ref{figbiaspara} suggests that the DR estimators are indeed consistent as the biases converge to 0 as the sample sizes $N$ get larger -- this is further illustrated in figure \ref{figsqrtnpara}, as we notice that the rates of convergence of the biases are faster than that of $\sqrt{N}$. We also notice that the DR estimation with wrong propensity score has the poorest convergence out of the 4 estimations, which is in line with the inefficiency that was suggested for such specifications by \citet{kang}.
    \item Figure \ref{figCIpara} shows the empirical coverage of the 95\% confidence intervals of each estimations, which indeed converge to 95\% for the correctly specified DR estimations. Because the estimation of the standard error was specifically for correctly specified model, the coverage for the DR estimations with one wrong nuisance parameter are either too high or too low.
    \item Figure \ref{figSEpara} confirms the statement above, as we see the ratio of the standard deviation of the DR estimates to the average standard error estimator is close to 1 for correctly specified models, hence suggesting a correct calculation of the confidence interval bounds. As for the DR estimations with one wrong model, this ratio is above or below 1, suggesting that the standard error estimation is on average too low or too high, hence giving a CI coverage that is too low or too high for these cases as the calculations of the CI bounds are inaccurate.
    \item Figure \ref{fighistpara} suggests that we indeed see asymptotic normality of the distribution, centered at the true mean, of the DR estimators that have at least one correctly specified nuisance parameter
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/biaspara.png}
    \caption{Simulation results of bias for the various DR estimator specifications}
    \label{figbiaspara}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/sqrtnpara.png}
    \caption{Simulation results of $\sqrt{N}$bias for the various DR estimator specifications}
    \label{figsqrtnpara}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/CIpara.png}
    \caption{Simulation results of 95\% confidence intervals coverage for the various DR estimator specifications}
    \label{figCIpara}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/SEpara.png}
    \caption{Accuracy of \citet{lunceford_davidian} SE estimation for the various DR estimator specifications}
    \label{figSEpara}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/histpara.png}
    \caption{Histogram of simulation results for  sample size $N = 2000$}
    \label{fighistpara}
\end{figure}
Although these results suggest that the parametric approach for setting the nuisance parameters is a good way of specifying these models, as shown by the consistency of the estimates, in reality we were only able to set the models correctly because we knew the distribution of the data-generative method, and therefore the fact that there was an interactive term between $W_1,W_2$ in the logistic regression model. Had we been given this data set without any information on how it was generated, it would have been a lot more time-consuming to find the correct specification of at least one nuisance parameter. This is where the practicality of ensemble learners from Machine Learning tools seem to be a more efficient way of setting these models, which we will cover in the next section. Nevertheless, a important advantage of parametric DR estimation is the user-friendly aspect, as we have mentioned previously that logistic or linear regressions models are easy fitting methods to start with.

\clearpage
\section{Machine Learning approaches to optimise the DR estimator}

Parametric models for the propensity score and outcome regression give more certainty, thanks to their explicit formulae and theoretical backings, but we have seen in the simulation study above that even a slight misspecification can lead to biased estimations. However we can apply asymptotic results such as the central limit theorem and the law of large numbers for convenient asymptotic normality and confidence intervals (CI).

But convenience is over weighed by defectiveness under model misspecification as we mentioned, so the probability of the CI containing the true mean converges to 0 \citep{diaz}.

Alternatively, one can use Machine Learning models to try to alleviate this bias, as they often provide models that are data adaptive and therefore easier to get a correctly specified model. In fact, \citet{ps_SL} have shown that when the propensity score is incorrectly specified, the use of Machine Learning tools for specifying the outcome regression can still give an unbiased DR estimator, which is an advantage over the parametric scenario, as we had seen that an incorrect propensity score with correct outcome regression was still giving the worse DR estimator in terms of bias in section \ref{para_model}.

Some data adaptive methods include: random forests, support vector machines, kernel regression, outcome weighed learning, Q-learning, neural networks, ensembles of these methods, etc. However, these methods may not have quantifiable theoretical guarantees. In the following sections we will take a closer look at the use of Random Forests and Multi-layer Perceptron \todo{and maybe SVMs if it finishes running} with various data settings, and see if these data adaptive methods give better estimates compared to the results from parametric models, even accounting for higher model complexity by adding more covariates to the data.

\subsection{Using Random Forests}

Random forests are a beginner friendly Machine Learning tool that create a classification or regression model based on multiple randomly generated decision trees that are combined together to make a better prediction than that of a single decision tree. It is easy to use and fit thanks to pre-existing packages in Python and R, and they are also comfortable working with a mix of continuous and categorical features. However it is not the most time efficient tool, as the training of the models requires a non-negligible amount of computational power. In this section we will explore the use of Random Forests for fitting the nuisance parameters in DR estimation.


\todo[inline]{More explanation on RF}

\subsubsection{RF simulation 1: 2 features}

\subsubsection*{Simulation framework}

We use the same data generated in section \ref{para_model}, and this time we calculate DR estimators with various nuisance parameter settings that use logistic regression or random forests:
\begin{itemize}
    \item \texttt{DR logistic ps, forest om}: the propensity is a correctly specified logistic regression, and the outcome model is a random forest. Both model account for the interaction term between $W_1$ and $W_2$
    \item \texttt{DR logistic om, forest ps}: the outcome model is a correctly specified logistic regression, and the propensity score is a random forest.
    \item \texttt{DR forest}: both the propensity and the outcome model are random forests that account for the interaction term between $W_1$ and $W_2$.
    \item \texttt{DR wrong logistic om, forest ps}: the outcome model is an incorrectly specified logistic regression, and the propensity score is a correctly specified random forest.
    \item \texttt{DR wrong logistic ps, forest om}: the propensity is an incorrectly specified logistic regression, and the outcome model is a correctly specified random forest.
    \item \texttt{DR wrong forest om, forest ps}: the outcome model is an incorrectly specified random forest (i.e no interaction term), and the propensity score is a correctly specified random forest.
    \item \texttt{DR wrong forest ps, forest om}: the propensity is an incorrectly specified random forest (i.e no interaction term), and the outcome model is a correctly specified random forest.
    \item \texttt{DR wrong forest ps \& om}: both the propensity and the outcome model are incorrectly specified random forests.
    \item \texttt{DR exact}: both the propensity and outcome model are exactly specified by using the distributions from the data generative methods, added for comparison.
\end{itemize}

Each time a random forest is specified, it goes through a stratified 5-fold cross validation of forest parameters in order to input the parameters that give a forest with the best accuracy on the test sets. A summary of the parameters tested by the cross-validation is provided in table \ref{}.\\
\begin{table}[]
    \centering
\begin{tabular}{ |p{3cm}|p{3cm}|p{3cm}| }
 \hline
 \multicolumn{3}{|c|}{Random Forest Cross-validation: Parameter Grid} \\
 \hline
 Number of decision trees & Maximum tree depth (number of splits)  & Maximum    number of features considered at each branch split\\
 \hline
 600, 800, 1000& 10, 15, 20, 25, 30 & 1, 2 \\
 \hline 
\end{tabular}
\caption{Parameter grid of 5-fold cross-validation for Random Forests in simulation 1}
\end{table}

As before, we obtain an empirical bias by taking the sample mean of 2500 generated estimators of each sample size varying from 200 to 1200 rows. \\

\subsubsection*{Results}

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/biasRF.png}
    \caption{Simulation results of bias for the various DR estimator specifications}
    \label{figbiasRF}
\end{figure}

\begin{figure} 
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/sqrtnRF.png}
    \caption{Accuracy of \citet{lunceford_davidian} SE estimation for the various DR estimator specifications}
    \label{figsqrtnRF}
\end{figure}

We notice in figure \ref{figbiasRF} that the models with random forests still provide DR estimates with bias converging to 0, even when the propensity and outcome model are set using a forest that doesn't account for the interaction term. This suggests that the forest is able to recover the misspecification, making it a useful tool in setting the nuisance parameters. Based on the sample sizes considered, figure \ref{figsqrtnRF} also suggests that theses models have biases with rate of convergence to 0 faster than $N^{-1/2}$, which further illustrates the double robustness of the estimator.

We also notice that when the outcome model was a forest and the propensity was wrongly specified with a model that doesn't account for the interaction term, whether that was a logistic regression or a Random Forest, the bias still performs well, which is in line with what \citet{ps_SL} have shown about the recovery of unbiasedness with ML tools for the outcome model when dealing with a wrong propensity score. 

However these figures show an unusual result for the model that had a wrong logistic outcome model, paired with a forest propensity score;  whereas when the misspecification of the outcome model is through a forest, the bias still performs well (its brown line on the graph is overlapped with the grey line). As we can see in figure \ref{figsqrtnRF}, the bias of this estimate does not converge fast enough, as the plot suggest its rate of convergence to 0 is slower than $N^{-1/2}$, making this estimate inconsistent. This makes us wonder if a Random Forest on the propensity is not strong enough to recover the parametric misspecification of the outcome model.

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/CIRF.png}
    \caption{Simulation results of 95\% confidence intervals coverage for the various DR estimator specifications}
    \label{figCIRF}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/SERF.png}
    \caption{Accuracy of \citet{lunceford_davidian} SE estimation for the various DR estimator specifications}
    \label{figSERF}
\end{figure}

As for the CI coverage results of the various DR estimations in figure \ref{figCIRF}, we notice that their convergence is slower compared to the parametric setting in section \ref{para_model}, and perhaps getting results from larger sample sizes would have shown that they eventually reach the 95\% goal. Figure \ref{figSERF} shows that the standard deviation of the estimated means also converge to the estimated value of the standard error at a slower rate than the parametric setting, hence explaining the slowness in CI coverage convergence as well. However for the DR estimator with the wrong logistic outcome model and forest propensity, this CI coverage simply converges to 0 (with the standard error estimate diverging from the standard deviation in \ref{figSERF}), so we eventually don't contain the true value of the sample mean in the confidence intervals based on this specification of the DR estimator. Again, more exploration of DR estimations using a wrong parametric outcome model and a random forest propensity is needed to see why this phenomenon is happening.

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/histRF.png}
    \caption{Accuracy of \citet{lunceford_davidian} SE estimation for the various DR estimator specifications}
    \label{fighistRF}
\end{figure}

Going back to the DR estimates that were only specified with Random Forests, figure \ref{fighistRF} suggest asymptotic normality of the distribution of these estimates, like in the parametric setting. \\

Overall the use of Random Forests in this data setting has resulted in consistent DR estimates (with the exception of one estimate mentioned before). Moreover, misspecification of the models by omitting the interaction term was an issue that the Random Forests easily overcame, as seen in the similar consistency and asymptotic normality results of the RF models that did and didn't account for this interaction. This is a non-negligible upgrade from the parametric setting as it suggests that RFs can provide a safety cushion for model specification.

\clearpage
\subsubsection{RF simulation 2: 4 features}

This time we increase the complexity of our DR estimation by adding two new features to the data-generative method from the previous section. We now have $\mathbf{W} = (W_1,W_2,W_3,W_4)$ where $W_1,W_2$ are as before, $W_3$ follows a standard normal distribution and $W_4$ follows an exponential distribution with mean 1. The new true propensity score is 
\begin{align*}
    \pi(\mathbf{W}) = P(R = 1 |\mathbf{W} = (w_1,w_2, w_3,w_4)) = \expit(-w_1 + 2w_1w_2 - w_3 + 2w_3w_4)
\end{align*}
and the new true outcome model is 
\begin{align*}
    m(\mathbf{W}) &= E(Y|\mathbf{W}, R=1) \\
    & = \expit(0.2 - w_1 + 2w_1w_2 - w_3 + 2w_3w_4)
\end{align*}

We ran simulations using the same combination of nuisance parameter settings as in simulation 1 with 2 features, but this time when we refer to wrong specification, it means the model didn't account for the two interaction terms $W_1W_2$ and $W_3W_4$. The cross-validation for the Random Forests was also expanded so as to consider up to 4 features at a time in each split. 

\subsubsection*{Results}

Unlike the simulation with 2 features, this time with the addition of 2 new features we obtained estimates that had poor convergence rates and consistency problems. Figure \ref{figbiasRF_moreW} already suggests higher bias in the estimators and a deceleration of the bias rate of convergence compared to the simulation with 2 features. This is further illustrated in figure \ref{figsqrtnRF_moreW} as the divergence of the bias elevated to $N^{1/2}$ suggest that the rate of convergence to 0 is slower than $N^{-1/2}$. The only estimator that performed well in this setting, based on the sample sizes considered, is the estimator with a correct logistic outcome model and a Random Forest propensity score that accounts for the interactions terms (orange line in the plots). 

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/biasRF_moreW.png}
    \caption{Simulation results of bias for the various DR estimator specifications using Random Forest and extra features}
    \label{figbiasRF_moreW}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/sqrtnRF_moreW.png}
    \caption{Accuracy of \citet{lunceford_davidian} SE estimation for the various DR estimator specifications}
    \label{figsqrtnRF_moreW}
\end{figure}

Unsurprisingly, the convergence to 95\% CI coverage is also poor as seen in figure \ref{figCIRF_moreW}, in fact many of the estimators have a CI coverage converging to 0. This is further illustrated by the divergence of the SE estimate from the true deviation as seen in figure \ref{figSERF_moreW}. Even for the estimator that had a fast enough bias convergence (orange in plots), the convergence to a 95\% coverage of its CIs is relatively slow, and from sample size 1000 to 1200, there is a slight decrease in coverage. This might suggest a worsening of the CI coverage in larger samples.

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/CIRF_moreW.png}
    \caption{Simulation results of 95\% confidence intervals coverage for the various DR estimator specifications}
    \label{figCIRF_moreW}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/SERF_moreW.png}
    \caption{Accuracy of \citet{lunceford_davidian} SE estimation for the various DR estimator specifications}
    \label{figSERF_moreW}
\end{figure}

It is with no surprise that due to the slowness of the convergences, a normal distribution centered at the true mean hasn't been achieved for the largest sample size considered in the simulation, as seen in figure \ref{fighistRF_moreW}, in particular for the DR estimators involving forests only. 

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/histRF_moreW.png}
    \caption{Accuracy of \citet{lunceford_davidian} SE estimation for the various DR estimator specifications}
    \label{fighistRF_moreW}
\end{figure}

Overall, doubling the number of features has resulted in poor consistency and slow convergence of the DR estimates to the true value of the mean. We have encountered what is called the curse of dimensionality \citep{Wasserman2006}: by doubling the number of features the complexity of training the Random Forests overcomes correct classification or regression, leading to the poor convergence of our DR estimates to the true value. Had we chosen distributions with a smaller range of values for $W_3,W_4$, we might have alleviated the complexity of the classification problem. Hence, although we had found good results using Random Forests in the 2-features case, one must take into account the complexity of the dataset and classification/regression problem before choosing to use a Random Forests to set the nuisance parameters of a DR estimator.

\clearpage
\subsection{Using Multi-Layer Perceptrons}
\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.7\columnwidth]{figures/Screenshot 2021-05-26 at 19.21.43.png}
    \caption{Network diagram of a single layer perceptron, \citet{hastieESL}}
    \label{fignn}
\end{figure}

This section is motivated by the publication by \citet{setoguchi-nn} suggesting that Neural Networks reduce the bias of propensity scores compared to logistic regression, and hence should also alleviate the bias of the DR estimator. This time we are using Multilayer Perceptrons, a type of forward-feeding artificial neural network. As mentioned by \citet{hastieESL}, there is a lot of popularity surrounding neural networks lately, and like Random Forests, they are easy to implement in Python thanks to the scikit-learn module. The training time of an MLP is also faster than that of a Random Forest for models with a small number of features. 

In a classification problem, there are $K$ output nodes at the end of the network diagram corresponding to the $K$ categories the output $Y$ can be in. The first layer of nodes in the diagram corresponds to the $p$ input features. Then, each node in the hidden layers of the network is created as a linear combination of the previous layer's nodes. At the end of the network, the output nodes are created as functions of linear combinations of the last hidden layer's nodes. An illustration of the network diagram of a single layer perceptron is provided in figure \ref{fignn}, taken from \textit{The Elements of Statistical Learning} by \citet{hastieESL}.

In the following sections we will evaluate the use of MLPs for DR estimation, through simulations on the dataset we have encountered in the previous sections.

\subsubsection{MLP simulation 1: 2 features}

We go back to the data structure that had 2 features $W_1,W_2$, adn we wish to see what happens to the consistency of the DR estimates when we use MLPs for the propensity score or outcome model. The various DR estimates based on MLP nuisance parameters are as follows: 
\begin{itemize}
    \item \texttt{DR logistic ps, MLP om}: the propensity is a correctly specified logistic regression, and the outcome model is a MLP. Both model account for the interaction term between $W_1$ and $W_2$
    \item \texttt{DR logistic om, MLP ps}: the outcome model is a correctly specified logistic regression, and the propensity score is a MLP.
    \item \texttt{DR MLP}: both the propensity and the outcome model are MLPs that account for the interaction term between $W_1$ and $W_2$.
    \item \texttt{DR wrong logistic om, MLP ps}: the outcome model is an incorrectly specified logistic regression, and the propensity score is a correctly specified MLP.
    \item \texttt{DR wrong logistic ps, MLP om}: the propensity is an incorrectly specified logistic regression, and the outcome model is a correctly specified MLP.
    \item \texttt{DR wrong MLP om, MLP ps}: the outcome model is an incorrectly specified MLP (i.e no interaction term), and the propensity score is a correctly specified MLP.
    \item \texttt{DR wrong MLP ps, MLP om}: the propensity is an incorrectly specified MLP (i.e no interaction term), and the outcome model is a correctly specified MLP.
    \item \texttt{DR wrong MLP ps \& om}: both the propensity and the outcome model are incorrectly specified MLPs.
    \item \texttt{DR exact}: both the propensity and outcome model are exactly specified by using the distributions from the data generative methods, added for comparison.
\end{itemize}

Like with Random Forests, the MLPs also go through a 5-fold cross-validation process to tune model parameters. A summary of the range of parameters tested by the cross-valdiation is summarised in table \ref{tableMLP}.

\begin{table}[h!]
    \centering
\begin{tabular}{ |p{3cm}|p{3cm}|p{3cm}|p{3cm}| }
 \hline
 \multicolumn{4}{|c|}{MLP Cross-validation: Parameter Grid} \\
 \hline
 Hidden layer sizes & Learning rate & Activation & Alpha\\
 \hline
 3 layers of size (10, 20, 30), 2 layers of size (50, 50), 1 layer of size 100 & Constant, invscaling, adaptive & Logistic, relu, tanh & $10^{-1}$, $10^{-2}$, $10^{-3}$, $10^{-4}$ \\
 \hline 
\end{tabular}
\caption{Parameter grid of 5-fold cross-validation for MLPS in simulation 1}
\label{tableMLP}
\end{table}

\subsubsection*{Results}

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/biasMLP.png}
    \caption{Simulation results of bias for the various DR estimator specifications}
    \label{figbiasMLP}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/sqrtnMLP.png}
    \caption{Accuracy of \citet{lunceford_davidian} SE estimation for the various DR estimator specifications}
    \label{figsqrtnMLP}
\end{figure}

In figure \ref{figbiasMLP} we notice that all estimates involving at least one MLP nuisance parameters have a bias seeming to converge towards zero. Excluding the DR estimate with exact specification, the estimate with lowest bias is the one that had a logistic outcome regression and a MLP propensity score, and it outperforms the estimate with logistic propensity score. This goes in line with the findings of \citet{setoguchi-nn} about an MLP propensity score having lower bias than with logistic regression. The DR estimate set with correct MLPs for both nuisance parameters performs well in terms of bias as well, although the convergence to 0 bias plateaus as sample sizes get larger. The DR estimate with wrong logistic outcome model and MLP propensity score (red line in plot) has a U-shaped bias curve, suggesting a worsening bias in larger samples. Looking at figure \ref{figsqrtnMLP}, the bias convergence to 0 for these estimates seem to be faster than that of $N^{-1/2}$ for most estimates as shown by the downward trend of the $\sqrt{N}$bias curves, while the estimate with wrong logistic outcome model and propensity MLP shows sign of diverging, in line with what we saw in figure \ref{figbiasMLP}. The DR estimator with wrong logistic propensity score and MLP outcome model (purple line on the plots) has one of highest bias among these estimators, but the convergence of the bias at a higher speed than $N^{-1/2}$ suggests that the incorrect specification was overcome by the MLPs and so the estimate is still consistent, even though it will converge towards the true mean at a slower pace than the estimates with at least one correct MLP specification. Another notable estimate is the one with wrong MLP propensity and MLP outcome model (pink line in plots). Even though this estimate starts at high bias, the latter is seeming to quickly converge to 0.

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/CIMLP.png}
    \caption{Simulation results of 95\% confidence intervals coverage for the various DR estimator specifications}
    \label{figCIMLP}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/SEMLP.png}
    \caption{Accuracy of \citet{lunceford_davidian} SE estimation for the various DR estimator specifications}
    \label{figSEMLP}
\end{figure}

The CI coverage plot in figure \ref{figCIMLP} also show interesting results. \texttt{DR logistic om, MLP ps}, \texttt{DR logistic ps, MLP om}, \texttt{DR wrong MLP om, MLP ps} \texttt{DR (in blue, orange and pink) have CI coverages tending to the 95\% goal, with the estimates with higher bias generally tending to 95\% coverage at a slower rate. The DR estimate struggling to get up to high CI coverage the most here is again the one with a MLP outcome model and wrong logistic propensity score. We see from figure \ref{figSEMLP} that non-surprisingly, its SE estimate is converging to the true standard deviation at a slow pace as well, given the considered sample sizes

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\columnwidth]{figures/histMLP.png}
    \caption{Accuracy of \citet{lunceford_davidian} SE estimation for the various DR estimator specifications}
    \label{fighistMLP}
\end{figure}

\clearpage
\subsubsection{Simulation 2: 4 features}

\todo[inline]{to be added once results come back}

\subsection{Comparison of the three methods}

\clearpage
\section{Discussion}

\section{Conclusion}

\clearpage
\section{Appendix}

\subsection*{Code for DR simulation}

\lstinputlisting[language=Python]{Code/DR_estimation.py}

%% bibliography
\clearpage
\bibliographystyle{unsrtnat}
\bibliography{bibliography}


\end{document}
